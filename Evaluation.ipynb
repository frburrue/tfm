{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparaci√≥n de datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pckl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/home/ubuntu/tfm/TrainYourOwnYOLO/Data/Source_Images/Training_Images/vott-csv-export-augmented-check/data_train.txt\"\n",
    "output_path = \"/home/ubuntu/tfm/TrainYourOwnYOLO/Data/Source_Images/Training_Images/vott-csv-export-augmented-check/data_test.pckl\"\n",
    "mapper = {1:'Panel', 0:'Dedo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open(input_path) as fd:\n",
    "    for item in fd:\n",
    "        filename_and_boxes = item.rstrip('\\n').split(' ')\n",
    "        filename = filename_and_boxes[0]\n",
    "        boxes = filename_and_boxes[1:]\n",
    "        d = {'filename': filename, 'object':[]}\n",
    "        for box in boxes:\n",
    "            box = box.split(',')\n",
    "            d['object'].append({'xmin':int(box[0]), 'ymin':int(box[1]), 'xmax': int(box[2]), 'ymax': int(box[3]), 'name': mapper[int(box[4])]})\n",
    "        rows.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pckl.dump(rows, open(output_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import pickle as pckl\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from scipy.special import expit\n",
    "from yolo3.yolo import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, classes_path, anchors_path):\n",
    "\n",
    "    yolo = YOLO(\n",
    "        **{\n",
    "            \"model_path\": model_path,\n",
    "            \"anchors_path\": anchors_path,\n",
    "            \"classes_path\": classes_path,\n",
    "            \"score\": 0.25,\n",
    "            \"gpu_num\": 1,\n",
    "            \"model_image_size\": (416, 416),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, c = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        \n",
    "        self.c       = c\n",
    "        self.classes = classes\n",
    "\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "        \n",
    "        return self.label\n",
    "    \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "            \n",
    "        return self.score \n",
    "\n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "             return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3  \n",
    "        \n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])  \n",
    "    \n",
    "    intersect = intersect_w * intersect_h\n",
    "\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    \n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    \n",
    "    return float(intersect) / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador de lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator():\n",
    "    def __init__(self, \n",
    "        instances, \n",
    "        anchors,   \n",
    "        labels,        \n",
    "        downsample=32, # ratio between network input's size and network output's size, 32 for YOLOv3\n",
    "        max_box_per_image=30,\n",
    "        batch_size=1,\n",
    "        min_net_size=320,\n",
    "        max_net_size=608,    \n",
    "        shuffle=True,\n",
    "        norm=None\n",
    "    ):\n",
    "        self.instances          = instances\n",
    "        self.batch_size         = batch_size\n",
    "        self.labels             = labels\n",
    "        self.downsample         = downsample\n",
    "        self.max_box_per_image  = max_box_per_image\n",
    "        self.min_net_size       = (min_net_size//self.downsample)*self.downsample\n",
    "        self.max_net_size       = (max_net_size//self.downsample)*self.downsample\n",
    "        self.shuffle            = shuffle\n",
    "        self.norm               = norm\n",
    "        self.anchors            = [BoundBox(0, 0, anchors[2*i], anchors[2*i+1]) for i in range(len(anchors)//2)]\n",
    "        self.net_h              = 416  \n",
    "        self.net_w              = 416\n",
    "\n",
    "        if shuffle: np.random.shuffle(self.instances)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(float(len(self.instances))/self.batch_size))           \n",
    "            \n",
    "    def num_classes(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.instances)    \n",
    "\n",
    "    def get_anchors(self):\n",
    "        anchors = []\n",
    "\n",
    "        for anchor in self.anchors:\n",
    "            anchors += [anchor.xmax, anchor.ymax]\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def load_annotation(self, i):\n",
    "        annots = []\n",
    "\n",
    "        for obj in self.instances[i]['object']:\n",
    "            annot = [obj['xmin'], obj['ymin'], obj['xmax'], obj['ymax'], self.labels.index(obj['name'])]\n",
    "            annots += [annot]\n",
    "\n",
    "        if len(annots) == 0: annots = [[]]\n",
    "\n",
    "        return np.array(annots)\n",
    "\n",
    "    def load_image(self, i):\n",
    "        return cv2.imread(self.instances[i]['filename'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, \n",
    "             generator, \n",
    "             iou_threshold=0.5,\n",
    "             obj_thresh=0.5,\n",
    "             nms_thresh=0.45,\n",
    "             net_h=416,\n",
    "             net_w=416,\n",
    "             save_path=None):\n",
    "    \"\"\" Evaluate a given dataset using a given model.\n",
    "    code originally from https://github.com/fizyr/keras-retinanet\n",
    "\n",
    "    # Arguments\n",
    "        model           : The model to evaluate.\n",
    "        generator       : The generator that represents the dataset to evaluate.\n",
    "        iou_threshold   : The threshold used to consider when a detection is positive or negative.\n",
    "        obj_thresh      : The threshold used to distinguish between object and non-object\n",
    "        nms_thresh      : The threshold used to determine whether two detections are duplicates\n",
    "        net_h           : The height of the input image to the model, higher value results in better accuracy\n",
    "        net_w           : The width of the input image to the model\n",
    "        save_path       : The path to save images with visualized detections to.\n",
    "    # Returns\n",
    "        A dict mapping class names to mAP scores.\n",
    "    \"\"\"    \n",
    "    # gather all detections and annotations\n",
    "    all_detections     = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "    all_annotations    = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "\n",
    "    for i in range(generator.size()):\n",
    "        raw_image = [generator.load_image(i)]\n",
    "\n",
    "        # make the boxes and the labels\n",
    "        pred_boxes = get_yolo_boxes(model, raw_image, net_h, net_w, generator.get_anchors(), obj_thresh, nms_thresh)[0]\n",
    "\n",
    "        score = np.array([box.get_score() for box in pred_boxes])\n",
    "        pred_labels = np.array([box.label for box in pred_boxes])        \n",
    "        \n",
    "        if len(pred_boxes) > 0:\n",
    "            pred_boxes = np.array([[box.xmin, box.ymin, box.xmax, box.ymax, box.get_score()] for box in pred_boxes]) \n",
    "        else:\n",
    "            pred_boxes = np.array([[]])  \n",
    "        \n",
    "        # sort the boxes and the labels according to scores\n",
    "        score_sort = np.argsort(-score)\n",
    "        pred_labels = pred_labels[score_sort]\n",
    "        pred_boxes  = pred_boxes[score_sort]\n",
    "        \n",
    "        # copy detections to all_detections\n",
    "        for label in range(generator.num_classes()):\n",
    "            all_detections[i][label] = pred_boxes[pred_labels == label, :]\n",
    "\n",
    "        annotations = generator.load_annotation(i)\n",
    "        \n",
    "        # copy detections to all_annotations\n",
    "        for label in range(generator.num_classes()):\n",
    "            all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy()\n",
    "\n",
    "    # compute mAP by comparing all detections and all annotations\n",
    "    average_precisions = {}\n",
    "    \n",
    "    for label in range(generator.num_classes()):\n",
    "        false_positives = np.zeros((0,))\n",
    "        true_positives  = np.zeros((0,))\n",
    "        scores          = np.zeros((0,))\n",
    "        num_annotations = 0.0\n",
    "\n",
    "        for i in range(generator.size()):\n",
    "            detections           = all_detections[i][label]\n",
    "            annotations          = all_annotations[i][label]\n",
    "            num_annotations     += annotations.shape[0]\n",
    "            detected_annotations = []\n",
    "\n",
    "            for d in detections:\n",
    "                scores = np.append(scores, d[4])\n",
    "\n",
    "                if annotations.shape[0] == 0: # Si no hay anotaciÔøΩn de esa detecciÔøΩn es un falso positivo\n",
    "                    false_positives = np.append(false_positives, 1) # Indicador de falso positivo\n",
    "                    true_positives  = np.append(true_positives, 0) # Indicador de ausencia de falso negativo\n",
    "                    continue\n",
    "\n",
    "                overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations) # IOI\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                max_overlap         = overlaps[0, assigned_annotation]\n",
    "\n",
    "                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
    "                    false_positives = np.append(false_positives, 0)\n",
    "                    true_positives  = np.append(true_positives, 1)\n",
    "                    detected_annotations.append(assigned_annotation)\n",
    "                else:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "\n",
    "        # no annotations -> AP for this class is 0 (is this correct?)\n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0\n",
    "            continue\n",
    "\n",
    "        # sort by score\n",
    "        indices         = np.argsort(-scores)\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives  = true_positives[indices]\n",
    "\n",
    "        # compute false positives and true positives\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives  = np.cumsum(true_positives)\n",
    "\n",
    "        # compute recall and precision\n",
    "        recall    = true_positives / num_annotations\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # compute average precision\n",
    "        average_precision  = compute_ap(recall, precision)\n",
    "        average_precisions[label] = {'AP': average_precision, 'recall': recall, 'precision': precision, 'f1': f1, 'support': 0}\n",
    "\n",
    "    return average_precisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ubuntu/tfm')\n",
    "config_path = './utils/config.json'\n",
    "\n",
    "with open(config_path) as config_buffer:    \n",
    "    config = json.loads(config_buffer.read())\n",
    "\n",
    "instances = pckl.load(open(config['model']['dataset_folder'], 'rb'))\n",
    "labels = config['model']['labels']\n",
    "labels = sorted(labels)\n",
    "\n",
    "valid_generator = BatchGenerator(\n",
    "    instances           = instances,\n",
    "    anchors             = config['model']['anchors'],   \n",
    "    labels              = sorted(config['model']['labels']),\n",
    "    downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n",
    "    max_box_per_image   = 0,\n",
    "    batch_size          = config['train']['batch_size'],\n",
    "    min_net_size        = config['model']['min_input_size'],\n",
    "    max_net_size        = config['model']['max_input_size'],   \n",
    "    shuffle             = True, \n",
    "    jitter              = 0.0, \n",
    "    norm                = None\n",
    ")\n",
    "\n",
    "infer_model = load_model(config['train']['model_folder'], config['train']['classes_path'], config['train']['anchors_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precisions = evaluate(infer_model, valid_generator)\n",
    "\n",
    "# print the score\n",
    "for label, average_precision in average_precisions.items():\n",
    "    print(labels[label] + ': {:.4f}'.format(average_precision['AP']))\n",
    "print('mAP: {:.4f}'.format(sum(average_precisions['AP'].values()) / len(average_precisions['AP'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
