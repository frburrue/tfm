{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pckl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/home/ubuntu/tfm/TrainYourOwnYOLO/Data/Source_Images/Training_Images/vott-csv-export-new-parsed/data_train_night.txt\"\n",
    "output_path = \"/home/ubuntu/tfm/TrainYourOwnYOLO/Data/Source_Images/Training_Images/vott-csv-export-new-parsed/data_test_night.pckl\"\n",
    "mapper = {0:'Panel', 1:'Dedo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open(input_path) as fd:\n",
    "    for item in fd:\n",
    "        filename_and_boxes = item.rstrip('\\n').split(' ')\n",
    "        filename = filename_and_boxes[0]\n",
    "        boxes = filename_and_boxes[1:]\n",
    "        d = {'filename': filename, 'object':[]}\n",
    "        for box in boxes:\n",
    "            box = box.split(',')\n",
    "            d['object'].append({'xmin':int(box[0]), 'ymin':int(box[1]), 'xmax': int(box[2]), 'ymax': int(box[3]), 'name': mapper[int(box[4])]})\n",
    "        rows.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pckl.dump(rows, open(output_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import pickle as pckl\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.special import expit\n",
    "from yolo3.yolo import YOLO\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, classes_path, anchors_path):\n",
    "\n",
    "    yolo = YOLO(\n",
    "        **{\n",
    "            \"model_path\": model_path,\n",
    "            \"anchors_path\": anchors_path,\n",
    "            \"classes_path\": classes_path,\n",
    "            \"score\": 0.5,\n",
    "            \"gpu_num\": 1,\n",
    "            \"model_image_size\": (416, 416),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, c = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        \n",
    "        self.c       = c\n",
    "        self.classes = classes\n",
    "\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "        \n",
    "        return self.label\n",
    "    \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "            \n",
    "        return self.score \n",
    "\n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "             return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3  \n",
    "        \n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])  \n",
    "    \n",
    "    intersect = intersect_w * intersect_h\n",
    "\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    \n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    \n",
    "    return float(intersect) / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador de lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator():\n",
    "    def __init__(self, instances, anchors, labels, batch_size=1, shuffle=True):\n",
    "        self.instances          = instances\n",
    "        self.batch_size         = batch_size\n",
    "        self.labels             = labels\n",
    "        self.anchors            = [BoundBox(0, 0, anchors[2*i], anchors[2*i+1]) for i in range(len(anchors)//2)]\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.instances)      \n",
    "            \n",
    "    def num_classes(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.instances)    \n",
    "\n",
    "    def get_anchors(self):\n",
    "        anchors = []\n",
    "\n",
    "        for anchor in self.anchors:\n",
    "            anchors += [anchor.xmax, anchor.ymax]\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def load_annotation(self, i):\n",
    "        annots = []\n",
    "\n",
    "        for obj in self.instances[i]['object']:\n",
    "            annot = [obj['xmin'], obj['ymin'], obj['xmax'], obj['ymax'], self.labels.index(obj['name'])]\n",
    "            annots += [annot]\n",
    "\n",
    "        if len(annots) == 0: annots = [[]]\n",
    "\n",
    "        return np.array(annots)\n",
    "\n",
    "    def load_image(self, i):\n",
    "        return cv2.imread(self.instances[i]['filename'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nms(boxes, nms_thresh):\n",
    "    if len(boxes) > 0:\n",
    "        nb_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "        \n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "\n",
    "            if boxes[index_i].classes[c] == 0: continue\n",
    "\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0\n",
    "                    \n",
    "def get_yolo_boxes(model, images, net_h, net_w, nms_thresh):\n",
    "    batch_output, data = model.detect_image(Image.fromarray(images[0].astype('uint8')))\n",
    "    boxes = []\n",
    "    \n",
    "    for bo in batch_output:\n",
    "        b = [0]*2\n",
    "        b[bo[4]] = bo[5]\n",
    "        box = bo[:4] + [bo[5]] + [b]\n",
    "        boxes.append(BoundBox(box[0], box[1], box[2], box[3], box[4], box[5]))\n",
    "\n",
    "    # image_h, image_w, _ = images[0].shape\n",
    "    # correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w)\n",
    "\n",
    "    do_nms(boxes, nms_thresh)\n",
    "    return [boxes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(model, generator, nms_thresh=0.5, net_h=416, net_w=416):  \n",
    "    # gather all detections and annotations\n",
    "    all_detections     = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "    all_annotations    = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "\n",
    "    for i in range(generator.size()):\n",
    "        raw_image = [generator.load_image(i)]\n",
    "\n",
    "        # make the boxes and the labels\n",
    "        pred_boxes = get_yolo_boxes(model, raw_image, net_h, net_w, nms_thresh)[0]\n",
    "\n",
    "        score = np.array([box.get_score() for box in pred_boxes])\n",
    "        pred_labels = np.array([box.label for box in pred_boxes])        \n",
    "        \n",
    "        if len(pred_boxes) > 0:\n",
    "            pred_boxes = np.array([[box.xmin, box.ymin, box.xmax, box.ymax, box.get_score()] for box in pred_boxes]) \n",
    "        else:\n",
    "            pred_boxes = np.array([[]])  \n",
    "        \n",
    "        # sort the boxes and the labels according to scores\n",
    "        score_sort = np.argsort(-score)\n",
    "        pred_labels = pred_labels[score_sort]\n",
    "        pred_boxes  = pred_boxes[score_sort]\n",
    "        \n",
    "        # copy detections to all_detections\n",
    "        for label in range(generator.num_classes()):\n",
    "            all_detections[i][label] = pred_boxes[pred_labels == label, :]\n",
    "\n",
    "        annotations = generator.load_annotation(i)\n",
    "        \n",
    "        # copy detections to all_annotations\n",
    "        for label in range(generator.num_classes()):\n",
    "            all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy()\n",
    "            \n",
    "    return all_detections, all_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap(a, b):\n",
    "    \"\"\"\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    Parameters\n",
    "    ----------\n",
    "    a: (N, 4) ndarray of float\n",
    "    b: (K, 4) ndarray of float\n",
    "    Returns\n",
    "    -------\n",
    "    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "\n",
    "    iw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n",
    "    ih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n",
    "\n",
    "    iw = np.maximum(iw, 0)\n",
    "    ih = np.maximum(ih, 0)\n",
    "\n",
    "    ua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n",
    "\n",
    "    ua = np.maximum(ua, np.finfo(float).eps)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    return intersection / ua  \n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(all_detections, all_annotations, generator, iou_threshold=0.5):\n",
    "    average_precisions = []\n",
    "    \n",
    "    for label in range(generator.num_classes()):\n",
    "        false_positives = np.zeros((0,))\n",
    "        true_positives  = np.zeros((0,))\n",
    "        scores          = np.zeros((0,))\n",
    "        num_annotations = 0.0\n",
    "\n",
    "        for i in range(generator.size()):\n",
    "            detections           = all_detections[i][label]\n",
    "            annotations          = all_annotations[i][label]\n",
    "            num_annotations     += annotations.shape[0]\n",
    "            detected_annotations = []\n",
    "\n",
    "            for d in detections:\n",
    "                scores = np.append(scores, d[4])\n",
    "\n",
    "                if annotations.shape[0] == 0: # Si no hay anotación de esa detección es un falso positivo\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                    continue\n",
    "\n",
    "                overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations) # IOU, tiene el consideración todas las anotaciones\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1) # Se queda con la anotación que maximiza el IOU\n",
    "                max_overlap         = overlaps[0, assigned_annotation] # Se queda con el valor del IOU se esta anotación\n",
    "\n",
    "                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations: # Comprueba si esa anotación no ha sido ya asignada a una detección (además de comprobar que el IOU supera un cierto umbral). Las detecciones están ordenadas por score descendente por lo que se quedaría primero la que tiene mayor score (aunque luego pueda tener menor IoU).\n",
    "                    false_positives = np.append(false_positives, 0)\n",
    "                    true_positives  = np.append(true_positives, 1)\n",
    "                    detected_annotations.append(assigned_annotation) # Guarda la anotación para que no pueda volver a ser usada\n",
    "                else: # IOU por debajo del umbral o anotación ya utilizada\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0) \n",
    "\n",
    "        # no annotations -> AP for this class is 0 (is this correct?)\n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0\n",
    "            continue\n",
    "\n",
    "        # sort by score (Esto lo hace para ser consistente con los vectores de anotación y detección)\n",
    "        indices         = np.argsort(-scores)\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives  = true_positives[indices]\n",
    "        annotations_pending = num_annotations - np.sum(true_positives)\n",
    "\n",
    "        # compute false positives and true positives (Esto es lo mismo que sumar unos y ceros de cada una de los vectores pero se hace así para computar el AP)\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives  = np.cumsum(true_positives)\n",
    "\n",
    "        # compute recall and precision (Y el F1)\n",
    "        recall    = true_positives / num_annotations # Es lo mismo que dividir entre TP + FN porque la suma de ambas tiene que ser el número de anotaciones (se detecten o no)\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # compute average precision\n",
    "        average_precision  = compute_ap(recall, precision)\n",
    "        average_precisions.append({'label': generator.labels[label], 'AP': average_precision, 'recall': recall[-1] if len(recall) else -1, 'precision': precision[-1] if len(precision) else -1, 'support': num_annotations, 'TP':true_positives[-1] if len(true_positives) else -1, 'FP': false_positives[-1] if len(false_positives) else -1, 'FN': annotations_pending})\n",
    "\n",
    "    return average_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_v2(all_detections, all_annotations, generator, confidence_threshold = 0.5, iou_threshold=0.5):\n",
    "    average_precisions = []\n",
    "    \n",
    "    for label in range(generator.num_classes()):\n",
    "        false_positives = np.zeros((0,))\n",
    "        true_positives  = np.zeros((0,))\n",
    "        false_negatives = np.zeros((0,))\n",
    "        true_negatives  = np.zeros((0,))\n",
    "        scores          = np.zeros((0,))\n",
    "        num_annotations = 0.0\n",
    "        annotations_pending = 0.0\n",
    "\n",
    "        for i in range(generator.size()):\n",
    "            detections           = all_detections[i][label]\n",
    "            annotations          = all_annotations[i][label]\n",
    "            num_annotations     += annotations.shape[0]\n",
    "            detected_annotations = []\n",
    "\n",
    "            for d in detections:\n",
    "                scores = np.append(scores, d[4])\n",
    "\n",
    "                if annotations.shape[0] == 0: # Si no hay anotación\n",
    "                    # index = int(not bool(label))\n",
    "                    # all_annotations[i][index].shape[0] == 0\n",
    "                    if d[4] < confidence_threshold: # El score no supera un umbral, verdadero negativo\n",
    "                        false_positives = np.append(false_positives, 0)\n",
    "                        true_positives  = np.append(true_positives, 0)\n",
    "                        false_negatives = np.append(false_negatives, 0)\n",
    "                        true_negatives  = np.append(true_negatives, 1) \n",
    "                    else: # El score es alto, falso positivo\n",
    "                        false_positives = np.append(false_positives, 1)\n",
    "                        true_positives  = np.append(true_positives, 0)\n",
    "                        false_negatives = np.append(false_negatives, 0)\n",
    "                        true_negatives  = np.append(true_negatives, 0)\n",
    "                    continue\n",
    "                    \n",
    "                overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations) # IOU, tiene el consideración todas las anotaciones\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1) # Se queda con la anotación que maximiza el IOU\n",
    "                max_overlap         = overlaps[0, assigned_annotation] # Se queda con el valor del IOU se esta anotación\n",
    "                \n",
    "                if assigned_annotation in detected_annotations:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                    false_negatives = np.append(false_negatives, 0)\n",
    "                    true_negatives  = np.append(true_negatives, 0)\n",
    "                    continue\n",
    "                \n",
    "                if max_overlap >= iou_threshold and d[4] >= confidence_threshold: # Comprueba que el IOU supera un cierto umbral de igual modo que la precisión en la clasificación debe superar otro\n",
    "                    false_positives = np.append(false_positives, 0)\n",
    "                    true_positives  = np.append(true_positives, 1)\n",
    "                    false_negatives = np.append(false_negatives, 0)\n",
    "                    true_negatives  = np.append(true_negatives, 0)\n",
    "                    detected_annotations.append(assigned_annotation)\n",
    "                else: # IOU por debajo del umbral o precisión en la clasificación por debajo de su umbral\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                    false_negatives = np.append(false_negatives, 0)\n",
    "                    true_negatives  = np.append(true_negatives, 0)\n",
    "                    \n",
    "            annotations_pending = (annotations_pending + annotations.shape[0]) - len(detected_annotations) \n",
    "            \n",
    "        # no annotations -> AP for this class is 0 (is this correct?)\n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0\n",
    "            continue\n",
    "\n",
    "        # sort by score (Esto lo hace para ser consistente con los vectores de anotación y detección)\n",
    "        indices         = np.argsort(-scores)\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives  = true_positives[indices]\n",
    "        true_negatives  = true_negatives[indices]\n",
    "\n",
    "        # compute false positives and true positives (Esto es lo mismo que sumar unos y ceros de cada una de los vectores pero se hace así para computar el AP)\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives  = np.cumsum(true_positives)\n",
    "        true_negatives  = np.cumsum(true_negatives)\n",
    "\n",
    "        # compute recall and precision (Y el F1)\n",
    "        recall    = true_positives / num_annotations # Es lo mismo que dividir entre TP + FN porque la suma de ambas tiene que ser el número de anotaciones (se detecten o no)\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # compute average precision\n",
    "        average_precision  = compute_ap(recall, precision)\n",
    "        average_precisions.append({'label': generator.labels[label], 'AP': average_precision, 'recall': recall[-1] if len(recall) else -1, 'precision': precision[-1] if len(precision) else -1, 'f1': f1[-1] if len(f1) else -1, 'support': num_annotations, 'TP':true_positives[-1], 'FP': false_positives[-1], 'TN': true_negatives[-1], 'FN': annotations_pending})\n",
    "\n",
    "    return average_precisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de modelo y de datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ubuntu/tfm')\n",
    "config_path = './utils/config.json'\n",
    "\n",
    "with open(config_path) as config_buffer:    \n",
    "    config = json.loads(config_buffer.read())\n",
    "\n",
    "instances = pckl.load(open(config['model']['dataset_folder'], 'rb'))\n",
    "labels = config['model']['labels']\n",
    "labels = sorted(labels)\n",
    "\n",
    "valid_generator = BatchGenerator(\n",
    "    instances           = instances,\n",
    "    anchors             = config['model']['anchors'],   \n",
    "    labels              = sorted(config['model']['labels']),\n",
    ")\n",
    "\n",
    "infer_model = load_model(config['train']['model_folder'], config['train']['classes_path'], config['train']['anchors_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detections, all_annotations = detection(infer_model, valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precisions = evaluation(all_detections, all_annotations, valid_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesar salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = 0\n",
    "precision = 0\n",
    "for average_precision in average_precisions:    \n",
    "    items += 1 \n",
    "    precision += average_precision['AP']\n",
    "display(pd.DataFrame(average_precisions))\n",
    "print('mAP: {:.4f}'.format(precision / items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precisions = evaluation_v2(all_detections, all_annotations, valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = 0\n",
    "precision = 0\n",
    "for average_precision in average_precisions:    \n",
    "    items += 1 \n",
    "    precision += average_precision['AP']\n",
    "display(pd.DataFrame(average_precisions))\n",
    "print('mAP: {:.4f}'.format(precision / items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "S3_CLIENT = boto3.resource('s3')\n",
    "\n",
    "mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI'))\n",
    "MLFLOW_CLIENT = mlflow.tracking.MlflowClient()\n",
    "\n",
    "REGISTERED_MODELS = [\"Hands\"]\n",
    "MODELS = {}\n",
    "\n",
    "\n",
    "def downlod_model(bucket_name, remoteDirectory_name):\n",
    "\n",
    "    bucket = S3_CLIENT.Bucket(bucket_name)\n",
    "\n",
    "    for obj in bucket.objects.filter(Prefix=remoteDirectory_name):\n",
    "        if not os.path.exists(os.path.dirname(obj.key)):\n",
    "            os.makedirs(os.path.dirname(obj.key))\n",
    "        bucket.download_file(obj.key, obj.key)\n",
    "\n",
    "\n",
    "def update_models(version=-1, remove_old_versions=True):\n",
    "\n",
    "    update = {}\n",
    "\n",
    "    for model_name in REGISTERED_MODELS:\n",
    "        model = None\n",
    "        update[model_name] = 0\n",
    "        for mv in MLFLOW_CLIENT.search_model_versions(f\"name='{model_name}'\"):\n",
    "            mv_bckp = mv\n",
    "            mv = dict(mv)\n",
    "            if version == mv['version'] or (version == -1 and mv['current_stage'] == 'Production'):\n",
    "                mv['last_updated_timestamp'] = str(datetime.fromtimestamp(int(mv['last_updated_timestamp'] / 1000)))\n",
    "                bucket = mv['source'].split('//')[1].split('/')[0]\n",
    "                folder = mv['source'].split('//')[1].split('/')[1]\n",
    "                if os.path.exists(os.path.join('./models', folder)):\n",
    "                    print(\"Load existing model...\")\n",
    "                    model = os.path.join(os.path.join('./models', folder), \"artifacts/model/data/model.h5\")\n",
    "                else:\n",
    "                    print(\"Downloading model...\")\n",
    "                    downlod_model(bucket, folder)\n",
    "                    model = os.path.join(os.path.join('./models', folder), \"artifacts/model/data/model.h5\")\n",
    "                    if remove_old_versions and os.path.exists('./models'):\n",
    "                        shutil.rmtree('./models')\n",
    "                    if not os.path.exists('./models'):\n",
    "                        os.mkdir('./models')\n",
    "                    shutil.move(os.path.join(os.getcwd(), folder), './models')\n",
    "                    update[model_name] = 1\n",
    "                print(\"Using model {name} v{version} ({current_stage}) updated at {last_updated_timestamp}\".format(**mv))\n",
    "                #response = {k: v for k, v in mv.items() if v}\n",
    "                break\n",
    "        if model:\n",
    "            MODELS[model_name] = (model, mv_bckp)\n",
    "\n",
    "    return update\n",
    "\n",
    "\n",
    "def get_model(model_name):\n",
    "\n",
    "    return MODELS.get(model_name, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/tfm/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "detection() got an unexpected keyword argument 'iou_threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-be5cdcd7298d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hands'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0minfer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classes_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anchors_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mall_detections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_annotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0maverage_precisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_detections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_annotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: detection() got an unexpected keyword argument 'iou_threshold'"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "os.chdir('/home/ubuntu/tfm/standalone')\n",
    "config_path = '../utils/config.json'\n",
    "\n",
    "with open(config_path) as config_buffer:    \n",
    "    config = json.loads(config_buffer.read())\n",
    "\n",
    "instances = pckl.load(open(config['model']['dataset_folder'], 'rb'))\n",
    "labels = config['model']['labels']\n",
    "labels = sorted(labels)\n",
    "\n",
    "valid_generator = BatchGenerator(\n",
    "    instances           = instances,\n",
    "    anchors             = config['model']['anchors'],   \n",
    "    labels              = sorted(config['model']['labels']),\n",
    ")\n",
    "\n",
    "versions = range(19,22)\n",
    "for iou in [0.6, 0.7, 0.8, 0.9]:\n",
    "    for version in tqdm(versions):\n",
    "        update_models(version)\n",
    "        model_path, model_meta = get_model('Hands')\n",
    "        infer_model = load_model(model_path, config['train']['classes_path'], config['train']['anchors_path'])\n",
    "        all_detections, all_annotations = detection(infer_model, valid_generator, iou_threshold=iou)\n",
    "        average_precisions = evaluation(all_detections, all_annotations, valid_generator)\n",
    "        items = 0\n",
    "        precision = 0\n",
    "        for average_precision in average_precisions:    \n",
    "            items += 1 \n",
    "            precision += average_precision['AP']\n",
    "        pckl.dump(((version,MLFLOW_CLIENT.get_run(model_meta.run_id)),(all_detections, all_annotations), (pd.DataFrame(average_precisions), 'mAP: {:.4f}'.format(precision / items))), open(f\"{version}_{iou}_.pckl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
