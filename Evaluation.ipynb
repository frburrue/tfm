{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparaci√≥n de datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pckl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/home/ubuntu/tfm/TrainYourOwnYOLO/Data/Source_Images/Training_Images/vott-csv-export-new-parsed/data_train_night.txt\"\n",
    "output_path = \"/home/ubuntu/tfm/TrainYourOwnYOLO/Data/Source_Images/Training_Images/vott-csv-export-new-parsed/data_test_night.pckl\"\n",
    "mapper = {0:'Panel', 1:'Dedo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open(input_path) as fd:\n",
    "    for item in fd:\n",
    "        filename_and_boxes = item.rstrip('\\n').split(' ')\n",
    "        filename = filename_and_boxes[0]\n",
    "        boxes = filename_and_boxes[1:]\n",
    "        d = {'filename': filename, 'object':[]}\n",
    "        for box in boxes:\n",
    "            box = box.split(',')\n",
    "            d['object'].append({'xmin':int(box[0]), 'ymin':int(box[1]), 'xmax': int(box[2]), 'ymax': int(box[3]), 'name': mapper[int(box[4])]})\n",
    "        rows.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pckl.dump(rows, open(output_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import pickle as pckl\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.special import expit\n",
    "from yolo3.yolo import YOLO\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, classes_path, anchors_path):\n",
    "\n",
    "    yolo = YOLO(\n",
    "        **{\n",
    "            \"model_path\": model_path,\n",
    "            \"anchors_path\": anchors_path,\n",
    "            \"classes_path\": classes_path,\n",
    "            \"score\": 0.5,\n",
    "            \"gpu_num\": 1,\n",
    "            \"model_image_size\": (416, 416),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, c = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        \n",
    "        self.c       = c\n",
    "        self.classes = classes\n",
    "\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "        \n",
    "        return self.label\n",
    "    \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "            \n",
    "        return self.score \n",
    "\n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "             return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3  \n",
    "        \n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])  \n",
    "    \n",
    "    intersect = intersect_w * intersect_h\n",
    "\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    \n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    \n",
    "    return float(intersect) / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador de lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator():\n",
    "    def __init__(self, instances, anchors, labels, batch_size=1, shuffle=True):\n",
    "        self.instances          = instances\n",
    "        self.batch_size         = batch_size\n",
    "        self.labels             = labels\n",
    "        self.anchors            = [BoundBox(0, 0, anchors[2*i], anchors[2*i+1]) for i in range(len(anchors)//2)]\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.instances)      \n",
    "            \n",
    "    def num_classes(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.instances)    \n",
    "\n",
    "    def get_anchors(self):\n",
    "        anchors = []\n",
    "\n",
    "        for anchor in self.anchors:\n",
    "            anchors += [anchor.xmax, anchor.ymax]\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def load_annotation(self, i):\n",
    "        annots = []\n",
    "\n",
    "        for obj in self.instances[i]['object']:\n",
    "            annot = [obj['xmin'], obj['ymin'], obj['xmax'], obj['ymax'], self.labels.index(obj['name'])]\n",
    "            annots += [annot]\n",
    "\n",
    "        if len(annots) == 0: annots = [[]]\n",
    "\n",
    "        return np.array(annots)\n",
    "\n",
    "    def load_image(self, i):\n",
    "        return cv2.imread(self.instances[i]['filename'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nms(boxes, nms_thresh):\n",
    "    if len(boxes) > 0:\n",
    "        nb_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "        \n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "\n",
    "            if boxes[index_i].classes[c] == 0: continue\n",
    "\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0\n",
    "                    \n",
    "def get_yolo_boxes(model, images, net_h, net_w, nms_thresh):\n",
    "    batch_output, data = model.detect_image(Image.fromarray(images[0].astype('uint8')))\n",
    "    boxes = []\n",
    "    \n",
    "    for bo in batch_output:\n",
    "        b = [0]*2\n",
    "        b[bo[4]] = bo[5]\n",
    "        box = bo[:4] + [bo[5]] + [b]\n",
    "        boxes.append(BoundBox(box[0], box[1], box[2], box[3], box[4], box[5]))\n",
    "\n",
    "    # image_h, image_w, _ = images[0].shape\n",
    "    # correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w)\n",
    "\n",
    "    do_nms(boxes, nms_thresh)\n",
    "    return [boxes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(model, generator, nms_thresh=0.5, net_h=416, net_w=416):  \n",
    "    # gather all detections and annotations\n",
    "    all_detections     = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "    all_annotations    = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "\n",
    "    for i in range(generator.size()):\n",
    "        raw_image = [generator.load_image(i)]\n",
    "\n",
    "        # make the boxes and the labels\n",
    "        pred_boxes = get_yolo_boxes(model, raw_image, net_h, net_w, nms_thresh)[0]\n",
    "\n",
    "        score = np.array([box.get_score() for box in pred_boxes])\n",
    "        pred_labels = np.array([box.label for box in pred_boxes])        \n",
    "        \n",
    "        if len(pred_boxes) > 0:\n",
    "            pred_boxes = np.array([[box.xmin, box.ymin, box.xmax, box.ymax, box.get_score()] for box in pred_boxes]) \n",
    "        else:\n",
    "            pred_boxes = np.array([[]])  \n",
    "        \n",
    "        # sort the boxes and the labels according to scores\n",
    "        score_sort = np.argsort(-score)\n",
    "        pred_labels = pred_labels[score_sort]\n",
    "        pred_boxes  = pred_boxes[score_sort]\n",
    "        \n",
    "        # copy detections to all_detections\n",
    "        for label in range(generator.num_classes()):\n",
    "            all_detections[i][label] = pred_boxes[pred_labels == label, :]\n",
    "\n",
    "        annotations = generator.load_annotation(i)\n",
    "        \n",
    "        # copy detections to all_annotations\n",
    "        for label in range(generator.num_classes()):\n",
    "            all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy()\n",
    "            \n",
    "    return all_detections, all_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap(a, b):\n",
    "    \"\"\"\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    Parameters\n",
    "    ----------\n",
    "    a: (N, 4) ndarray of float\n",
    "    b: (K, 4) ndarray of float\n",
    "    Returns\n",
    "    -------\n",
    "    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "\n",
    "    iw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n",
    "    ih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n",
    "\n",
    "    iw = np.maximum(iw, 0)\n",
    "    ih = np.maximum(ih, 0)\n",
    "\n",
    "    ua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n",
    "\n",
    "    ua = np.maximum(ua, np.finfo(float).eps)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    return intersection / ua  \n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(all_detections, all_annotations, generator, iou_threshold=0.5):\n",
    "    average_precisions = []\n",
    "    \n",
    "    for label in range(generator.num_classes()):\n",
    "        false_positives = np.zeros((0,))\n",
    "        true_positives  = np.zeros((0,))\n",
    "        scores          = np.zeros((0,))\n",
    "        num_annotations = 0.0\n",
    "\n",
    "        for i in range(generator.size()):\n",
    "            detections           = all_detections[i][label]\n",
    "            annotations          = all_annotations[i][label]\n",
    "            num_annotations     += annotations.shape[0]\n",
    "            detected_annotations = []\n",
    "\n",
    "            for d in detections:\n",
    "                scores = np.append(scores, d[4])\n",
    "\n",
    "                if annotations.shape[0] == 0: # Si no hay anotaci√≥n de esa detecci√≥n es un falso positivo\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                    continue\n",
    "\n",
    "                overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations) # IOU, tiene el consideraci√≥n todas las anotaciones\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1) # Se queda con la anotaci√≥n que maximiza el IOU\n",
    "                max_overlap         = overlaps[0, assigned_annotation] # Se queda con el valor del IOU se esta anotaci√≥n\n",
    "\n",
    "                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations: # Comprueba si esa anotaci√≥n no ha sido ya asignada a una detecci√≥n (adem√°s de comprobar que el IOU supera un cierto umbral). Las detecciones est√°n ordenadas por score descendente por lo que se quedar√≠a primero la que tiene mayor score (aunque luego pueda tener menor IoU).\n",
    "                    false_positives = np.append(false_positives, 0)\n",
    "                    true_positives  = np.append(true_positives, 1)\n",
    "                    detected_annotations.append(assigned_annotation) # Guarda la anotaci√≥n para que no pueda volver a ser usada\n",
    "                else: # IOU por debajo del umbral o anotaci√≥n ya utilizada\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0) \n",
    "\n",
    "        # no annotations -> AP for this class is 0 (is this correct?)\n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0\n",
    "            continue\n",
    "\n",
    "        # sort by score (Esto lo hace para ser consistente con los vectores de anotaci√≥n y detecci√≥n)\n",
    "        indices         = np.argsort(-scores)\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives  = true_positives[indices]\n",
    "        annotations_pending = num_annotations - np.sum(true_positives)\n",
    "\n",
    "        # compute false positives and true positives (Esto es lo mismo que sumar unos y ceros de cada una de los vectores pero se hace as√≠ para computar el AP)\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives  = np.cumsum(true_positives)\n",
    "\n",
    "        # compute recall and precision (Y el F1)\n",
    "        recall    = true_positives / num_annotations # Es lo mismo que dividir entre TP + FN porque la suma de ambas tiene que ser el n√∫mero de anotaciones (se detecten o no)\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # compute average precision\n",
    "        average_precision  = compute_ap(recall, precision)\n",
    "        average_precisions.append({'label': generator.labels[label], 'AP': average_precision, 'recall': recall[-1] if len(recall) else -1, 'precision': precision[-1] if len(precision) else -1, 'support': num_annotations, 'TP':true_positives[-1] if len(true_positives) else -1, 'FP': false_positives[-1] if len(false_positives) else -1, 'FN': annotations_pending})\n",
    "\n",
    "    return average_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_v2(all_detections, all_annotations, generator, confidence_threshold = 0.5, iou_threshold=0.5):\n",
    "    average_precisions = []\n",
    "    \n",
    "    for label in range(generator.num_classes()):\n",
    "        false_positives = np.zeros((0,))\n",
    "        true_positives  = np.zeros((0,))\n",
    "        false_negatives = np.zeros((0,))\n",
    "        true_negatives  = np.zeros((0,))\n",
    "        scores          = np.zeros((0,))\n",
    "        num_annotations = 0.0\n",
    "        annotations_pending = 0.0\n",
    "\n",
    "        for i in range(generator.size()):\n",
    "            detections           = all_detections[i][label]\n",
    "            annotations          = all_annotations[i][label]\n",
    "            num_annotations     += annotations.shape[0]\n",
    "            detected_annotations = []\n",
    "\n",
    "            for d in detections:\n",
    "                scores = np.append(scores, d[4])\n",
    "\n",
    "                if annotations.shape[0] == 0: # Si no hay anotaci√≥n\n",
    "                    # index = int(not bool(label))\n",
    "                    # all_annotations[i][index].shape[0] == 0\n",
    "                    if d[4] < confidence_threshold: # El score no supera un umbral, verdadero negativo\n",
    "                        false_positives = np.append(false_positives, 0)\n",
    "                        true_positives  = np.append(true_positives, 0)\n",
    "                        false_negatives = np.append(false_negatives, 0)\n",
    "                        true_negatives  = np.append(true_negatives, 1) \n",
    "                    else: # El score es alto, falso positivo\n",
    "                        false_positives = np.append(false_positives, 1)\n",
    "                        true_positives  = np.append(true_positives, 0)\n",
    "                        false_negatives = np.append(false_negatives, 0)\n",
    "                        true_negatives  = np.append(true_negatives, 0)\n",
    "                    continue\n",
    "                    \n",
    "                overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations) # IOU, tiene el consideraci√≥n todas las anotaciones\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1) # Se queda con la anotaci√≥n que maximiza el IOU\n",
    "                max_overlap         = overlaps[0, assigned_annotation] # Se queda con el valor del IOU se esta anotaci√≥n\n",
    "                \n",
    "                if assigned_annotation in detected_annotations:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                    false_negatives = np.append(false_negatives, 0)\n",
    "                    true_negatives  = np.append(true_negatives, 0)\n",
    "                    continue\n",
    "                \n",
    "                if max_overlap >= iou_threshold and d[4] >= confidence_threshold: # Comprueba que el IOU supera un cierto umbral de igual modo que la precisi√≥n en la clasificaci√≥n debe superar otro\n",
    "                    false_positives = np.append(false_positives, 0)\n",
    "                    true_positives  = np.append(true_positives, 1)\n",
    "                    false_negatives = np.append(false_negatives, 0)\n",
    "                    true_negatives  = np.append(true_negatives, 0)\n",
    "                    detected_annotations.append(assigned_annotation)\n",
    "                else: # IOU por debajo del umbral o precisi√≥n en la clasificaci√≥n por debajo de su umbral\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                    false_negatives = np.append(false_negatives, 0)\n",
    "                    true_negatives  = np.append(true_negatives, 0)\n",
    "                    \n",
    "            annotations_pending = (annotations_pending + annotations.shape[0]) - len(detected_annotations) \n",
    "            \n",
    "        # no annotations -> AP for this class is 0 (is this correct?)\n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0\n",
    "            continue\n",
    "\n",
    "        # sort by score (Esto lo hace para ser consistente con los vectores de anotaci√≥n y detecci√≥n)\n",
    "        indices         = np.argsort(-scores)\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives  = true_positives[indices]\n",
    "        true_negatives  = true_negatives[indices]\n",
    "\n",
    "        # compute false positives and true positives (Esto es lo mismo que sumar unos y ceros de cada una de los vectores pero se hace as√≠ para computar el AP)\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives  = np.cumsum(true_positives)\n",
    "        true_negatives  = np.cumsum(true_negatives)\n",
    "\n",
    "        # compute recall and precision (Y el F1)\n",
    "        recall    = true_positives / num_annotations # Es lo mismo que dividir entre TP + FN porque la suma de ambas tiene que ser el n√∫mero de anotaciones (se detecten o no)\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # compute average precision\n",
    "        average_precision  = compute_ap(recall, precision)\n",
    "        average_precisions.append({'label': generator.labels[label], 'AP': average_precision, 'recall': recall[-1] if len(recall) else -1, 'precision': precision[-1] if len(precision) else -1, 'f1': f1[-1] if len(f1) else -1, 'support': num_annotations, 'TP':true_positives[-1], 'FP': false_positives[-1], 'TN': true_negatives[-1], 'FN': annotations_pending})\n",
    "\n",
    "    return average_precisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de modelo y de datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ubuntu/tfm')\n",
    "config_path = './utils/config.json'\n",
    "\n",
    "with open(config_path) as config_buffer:    \n",
    "    config = json.loads(config_buffer.read())\n",
    "\n",
    "instances = pckl.load(open(config['model']['dataset_folder'], 'rb'))\n",
    "labels = config['model']['labels']\n",
    "labels = sorted(labels)\n",
    "\n",
    "valid_generator = BatchGenerator(\n",
    "    instances           = instances,\n",
    "    anchors             = config['model']['anchors'],   \n",
    "    labels              = sorted(config['model']['labels']),\n",
    ")\n",
    "\n",
    "infer_model = load_model(config['train']['model_folder'], config['train']['classes_path'], config['train']['anchors_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detections, all_annotations = detection(infer_model, valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precisions = evaluation(all_detections, all_annotations, valid_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesar salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = 0\n",
    "precision = 0\n",
    "for average_precision in average_precisions:    \n",
    "    items += 1 \n",
    "    precision += average_precision['AP']\n",
    "display(pd.DataFrame(average_precisions))\n",
    "print('mAP: {:.4f}'.format(precision / items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precisions = evaluation_v2(all_detections, all_annotations, valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = 0\n",
    "precision = 0\n",
    "for average_precision in average_precisions:    \n",
    "    items += 1 \n",
    "    precision += average_precision['AP']\n",
    "display(pd.DataFrame(average_precisions))\n",
    "print('mAP: {:.4f}'.format(precision / items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "S3_CLIENT = boto3.resource('s3')\n",
    "\n",
    "mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI'))\n",
    "MLFLOW_CLIENT = mlflow.tracking.MlflowClient()\n",
    "\n",
    "REGISTERED_MODELS = [\"Hands\"]\n",
    "MODELS = {}\n",
    "\n",
    "\n",
    "def downlod_model(bucket_name, remoteDirectory_name):\n",
    "\n",
    "    bucket = S3_CLIENT.Bucket(bucket_name)\n",
    "\n",
    "    for obj in bucket.objects.filter(Prefix=remoteDirectory_name):\n",
    "        if not os.path.exists(os.path.dirname(obj.key)):\n",
    "            os.makedirs(os.path.dirname(obj.key))\n",
    "        bucket.download_file(obj.key, obj.key)\n",
    "\n",
    "\n",
    "def update_models(version=-1, remove_old_versions=True):\n",
    "\n",
    "    update = {}\n",
    "\n",
    "    for model_name in REGISTERED_MODELS:\n",
    "        model = None\n",
    "        update[model_name] = 0\n",
    "        for mv in MLFLOW_CLIENT.search_model_versions(f\"name='{model_name}'\"):\n",
    "            mv_bckp = mv\n",
    "            mv = dict(mv)\n",
    "            if version == mv['version'] or (version == -1 and mv['current_stage'] == 'Production'):\n",
    "                mv['last_updated_timestamp'] = str(datetime.fromtimestamp(int(mv['last_updated_timestamp'] / 1000)))\n",
    "                bucket = mv['source'].split('//')[1].split('/')[0]\n",
    "                folder = mv['source'].split('//')[1].split('/')[1]\n",
    "                if os.path.exists(os.path.join('./models', folder)):\n",
    "                    print(\"Load existing model...\")\n",
    "                    model = os.path.join(os.path.join('./models', folder), \"artifacts/model/data/model.h5\")\n",
    "                else:\n",
    "                    print(\"Downloading model...\")\n",
    "                    downlod_model(bucket, folder)\n",
    "                    model = os.path.join(os.path.join('./models', folder), \"artifacts/model/data/model.h5\")\n",
    "                    if remove_old_versions and os.path.exists('./models'):\n",
    "                        shutil.rmtree('./models')\n",
    "                    if not os.path.exists('./models'):\n",
    "                        os.mkdir('./models')\n",
    "                    shutil.move(os.path.join(os.getcwd(), folder), './models')\n",
    "                    update[model_name] = 1\n",
    "                print(\"Using model {name} v{version} ({current_stage}) updated at {last_updated_timestamp}\".format(**mv))\n",
    "                #response = {k: v for k, v in mv.items() if v}\n",
    "                break\n",
    "        if model:\n",
    "            MODELS[model_name] = (model, mv_bckp)\n",
    "\n",
    "    return update\n",
    "\n",
    "\n",
    "def get_model(model_name):\n",
    "\n",
    "    return MODELS.get(model_name, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/tfm/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "detection() got an unexpected keyword argument 'iou_threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-be5cdcd7298d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hands'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0minfer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classes_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anchors_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mall_detections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_annotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0maverage_precisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_detections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_annotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: detection() got an unexpected keyword argument 'iou_threshold'"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "os.chdir('/home/ubuntu/tfm/standalone')\n",
    "config_path = '../utils/config.json'\n",
    "\n",
    "with open(config_path) as config_buffer:    \n",
    "    config = json.loads(config_buffer.read())\n",
    "\n",
    "instances = pckl.load(open(config['model']['dataset_folder'], 'rb'))\n",
    "labels = config['model']['labels']\n",
    "labels = sorted(labels)\n",
    "\n",
    "valid_generator = BatchGenerator(\n",
    "    instances           = instances,\n",
    "    anchors             = config['model']['anchors'],   \n",
    "    labels              = sorted(config['model']['labels']),\n",
    ")\n",
    "\n",
    "versions = range(19,22)\n",
    "for iou in [0.6, 0.7, 0.8, 0.9]:\n",
    "    for version in tqdm(versions):\n",
    "        update_models(version)\n",
    "        model_path, model_meta = get_model('Hands')\n",
    "        infer_model = load_model(model_path, config['train']['classes_path'], config['train']['anchors_path'])\n",
    "        all_detections, all_annotations = detection(infer_model, valid_generator, iou_threshold=iou)\n",
    "        average_precisions = evaluation(all_detections, all_annotations, valid_generator)\n",
    "        items = 0\n",
    "        precision = 0\n",
    "        for average_precision in average_precisions:    \n",
    "            items += 1 \n",
    "            precision += average_precision['AP']\n",
    "        pckl.dump(((version,MLFLOW_CLIENT.get_run(model_meta.run_id)),(all_detections, all_annotations), (pd.DataFrame(average_precisions), 'mAP: {:.4f}'.format(precision / items))), open(f\"{version}_{iou}_.pckl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
